{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14246e-f88a-48d5-9010-07e50c67d132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Duisek Bermagambet + Professor Ilyas Muhammad \n",
    "# HIL DDPG system\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# I use RTX 4060 locally\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cae791",
   "metadata": {},
   "source": [
    "Professor, I opted to start with using basic GPS data instead of visual, since I've been strugling to properly improve existin approaches in 3D spaces.\n",
    "In this case I generate 2D scenarios for drones to operate in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626609dc-aa48-4701-8e34-7c064c25e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneGPSDataGenerator:    \n",
    "    def __init__(self, area_size: Tuple[float, float] = (1000, 1000), \n",
    "                 n_obstacles: int = 10, n_waypoints: int = 5):\n",
    "        self.area_size = area_size\n",
    "        self.n_obstacles = n_obstacles\n",
    "        self.n_waypoints = n_waypoints\n",
    "        self.obstacles = self._generate_obstacles()\n",
    "        self.waypoints = self._generate_waypoints()\n",
    "        \n",
    "    def _generate_obstacles(self) -> List[Dict]:\n",
    "        obstacles = []\n",
    "        attempts = 0\n",
    "        \n",
    "        while len(obstacles) < self.n_obstacles and attempts < 100:\n",
    "            new_obs = {\n",
    "                'id': len(obstacles),\n",
    "                'position': (np.random.uniform(100, self.area_size[0]-100),\n",
    "                           np.random.uniform(100, self.area_size[1]-100)),\n",
    "                'radius': np.random.uniform(20, 40),\n",
    "                'type': np.random.choice(['building', 'no_fly_zone', 'weather_hazard'])\n",
    "            }\n",
    "            \n",
    "            valid = True\n",
    "            for obs in obstacles:\n",
    "                dist = np.sqrt((new_obs['position'][0] - obs['position'][0])**2 + \n",
    "                             (new_obs['position'][1] - obs['position'][1])**2)\n",
    "                if dist < (new_obs['radius'] + obs['radius'] + 50):  # 50m minimum separation\n",
    "                    valid = False\n",
    "                    break\n",
    "            \n",
    "            if valid:\n",
    "                obstacles.append(new_obs)\n",
    "            attempts += 1\n",
    "            \n",
    "        return obstacles\n",
    "    \n",
    "    def _generate_waypoints(self) -> List[Tuple[float, float]]:\n",
    "        waypoints = []\n",
    "        for _ in range(self.n_waypoints):\n",
    "            valid = False\n",
    "            while not valid:\n",
    "                pos = (np.random.uniform(50, self.area_size[0]-50),\n",
    "                      np.random.uniform(50, self.area_size[1]-50))\n",
    "                valid = True\n",
    "                for obs in self.obstacles:\n",
    "                    dist = np.sqrt((pos[0] - obs['position'][0])**2 + \n",
    "                                 (pos[1] - obs['position'][1])**2)\n",
    "                    if dist < obs['radius'] + 30:  # 30m safety margin\n",
    "                        valid = False\n",
    "                        break\n",
    "            waypoints.append(pos)\n",
    "        return waypoints\n",
    "    \n",
    "    def generate_trajectory_data(self, n_episodes: int = 100) -> pd.DataFrame:\n",
    "        data = []\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            start_pos = (np.random.uniform(0, self.area_size[0]),\n",
    "                        np.random.uniform(0, self.area_size[1]))\n",
    "            \n",
    "            current_pos = list(start_pos)\n",
    "            target_wp_idx = 0\n",
    "            \n",
    "            for t in range(200):\n",
    "                if target_wp_idx >= len(self.waypoints):\n",
    "                    break\n",
    "                    \n",
    "                target = self.waypoints[target_wp_idx]\n",
    "                \n",
    "                dx = target[0] - current_pos[0]\n",
    "                dy = target[1] - current_pos[1]\n",
    "                dist = np.sqrt(dx**2 + dy**2)\n",
    "                \n",
    "                if dist < 10:\n",
    "                    target_wp_idx += 1\n",
    "                    continue\n",
    "                \n",
    "                dx_norm = dx / dist + np.random.normal(0, 0.1)\n",
    "                dy_norm = dy / dist + np.random.normal(0, 0.1)\n",
    "                \n",
    "                current_pos[0] += dx_norm * 5\n",
    "                current_pos[1] += dy_norm * 5\n",
    "                \n",
    "                danger_level = 0\n",
    "                min_obstacle_dist = float('inf')\n",
    "                for obs in self.obstacles:\n",
    "                    obs_dist = np.sqrt((current_pos[0] - obs['position'][0])**2 + \n",
    "                                     (current_pos[1] - obs['position'][1])**2)\n",
    "                    min_obstacle_dist = min(min_obstacle_dist, obs_dist - obs['radius'])\n",
    "                    \n",
    "                    if obs_dist < obs['radius'] + 50:\n",
    "                        danger_level = max(danger_level, \n",
    "                                         1 - (obs_dist - obs['radius']) / 50)\n",
    "                \n",
    "                wind_speed = np.random.uniform(0, 20)\n",
    "                battery = 100 - (t / 200) * 50\n",
    "                altitude = 50 + np.random.normal(0, 2)\n",
    "                \n",
    "                data.append({\n",
    "                    'episode': episode,\n",
    "                    'timestep': t,\n",
    "                    'x': current_pos[0],\n",
    "                    'y': current_pos[1],\n",
    "                    'altitude': altitude,\n",
    "                    'target_x': target[0],\n",
    "                    'target_y': target[1],\n",
    "                    'velocity_x': dx_norm * 5,\n",
    "                    'velocity_y': dy_norm * 5,\n",
    "                    'wind_speed': wind_speed,\n",
    "                    'battery': battery,\n",
    "                    'danger_level': danger_level,\n",
    "                    'min_obstacle_distance': max(0, min_obstacle_dist),\n",
    "                    'waypoint_index': target_wp_idx\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7a025-6357-4623-a522-44d10819ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class AuthorityNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 128):\n",
    "        super(AuthorityNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Output between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de2c1c08-0190-4b7c-85ad-0df7bb14f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45eb9b-ff9b-48e9-9d93-8d3598a89822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneNavigationEnv:    \n",
    "    def __init__(self, gps_generator: DroneGPSDataGenerator):\n",
    "        self.gps_generator = gps_generator\n",
    "        self.state_dim = 12\n",
    "        self.action_dim = 3 \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_waypoint_idx = 0\n",
    "        self.timestep = 0\n",
    "        self.position = np.array([\n",
    "            np.random.uniform(0, self.gps_generator.area_size[0]),\n",
    "            np.random.uniform(0, self.gps_generator.area_size[1]),\n",
    "            50.0  # altitude\n",
    "        ])\n",
    "        self.velocity = np.zeros(3)\n",
    "        self.battery = 100.0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        if self.current_waypoint_idx < len(self.gps_generator.waypoints):\n",
    "            target = self.gps_generator.waypoints[self.current_waypoint_idx]\n",
    "            target_alt = 50.0\n",
    "        else:\n",
    "            target = (self.position[0], self.position[1])\n",
    "            target_alt = self.position[2]\n",
    "        \n",
    "        danger_level = 0\n",
    "        min_obs_dist = float('inf')\n",
    "        for obs in self.gps_generator.obstacles:\n",
    "            dist = np.sqrt((self.position[0] - obs['position'][0])**2 + \n",
    "                         (self.position[1] - obs['position'][1])**2)\n",
    "            min_obs_dist = min(min_obs_dist, dist - obs['radius'])\n",
    "            if dist < obs['radius'] + 50:\n",
    "                danger_level = max(danger_level, 1 - (dist - obs['radius']) / 50)\n",
    "        \n",
    "        state = np.array([\n",
    "            self.position[0] / 1000,  # Normalize\n",
    "            self.position[1] / 1000,\n",
    "            self.position[2] / 100,\n",
    "            target[0] / 1000,\n",
    "            target[1] / 1000,\n",
    "            target_alt / 100,\n",
    "            self.velocity[0] / 10,\n",
    "            self.velocity[1] / 10,\n",
    "            self.velocity[2] / 5,\n",
    "            np.random.uniform(0, 20) / 20,  # Wind\n",
    "            self.battery / 100,\n",
    "            danger_level\n",
    "        ])\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.velocity = np.clip(action * np.array([10, 10, 5]), -10, 10)\n",
    "        \n",
    "        self.position += self.velocity * 0.1  # 0.1s timestep\n",
    "        \n",
    "        self.position[0] = np.clip(self.position[0], 0, self.gps_generator.area_size[0])\n",
    "        self.position[1] = np.clip(self.position[1], 0, self.gps_generator.area_size[1])\n",
    "        self.position[2] = np.clip(self.position[2], 10, 150)\n",
    "        \n",
    "        self.battery -= 0.05 + np.linalg.norm(self.velocity) * 0.005\n",
    "        \n",
    "        if self.current_waypoint_idx < len(self.gps_generator.waypoints):\n",
    "            target = self.gps_generator.waypoints[self.current_waypoint_idx]\n",
    "            dist_to_target = np.sqrt((self.position[0] - target[0])**2 + \n",
    "                                   (self.position[1] - target[1])**2)\n",
    "            \n",
    "            if dist_to_target < 20:\n",
    "                self.current_waypoint_idx += 1\n",
    "                print(f\"Waypoint {self.current_waypoint_idx}/{len(self.gps_generator.waypoints)} reached!\")\n",
    "        else:\n",
    "            dist_to_target = 0\n",
    "        \n",
    "        reward = self._calculate_reward(dist_to_target)\n",
    "        \n",
    "        mission_complete = self.current_waypoint_idx >= len(self.gps_generator.waypoints)\n",
    "        done = (self.battery <= 0 or mission_complete or self.timestep >= 2000)\n",
    "        \n",
    "        if mission_complete and not hasattr(self, 'mission_completed'):\n",
    "            reward += 100  # Big bonus for completing mission\n",
    "            self.mission_completed = True\n",
    "        \n",
    "        self.timestep += 1\n",
    "        \n",
    "        info = {\n",
    "            'mission_complete': mission_complete,\n",
    "            'waypoints_reached': self.current_waypoint_idx,\n",
    "            'total_waypoints': len(self.gps_generator.waypoints)\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _calculate_reward(self, dist_to_target):\n",
    "        if self.current_waypoint_idx < len(self.gps_generator.waypoints):\n",
    "            dist_reward = -dist_to_target / 50\n",
    "        else:\n",
    "            dist_reward = 0\n",
    "        \n",
    "        wp_reward = 50 if dist_to_target < 20 else 0  # Increased reward\n",
    "        \n",
    "        progress_reward = self.current_waypoint_idx * 5\n",
    "        \n",
    "        safety_penalty = 0\n",
    "        min_safe_distance = float('inf')\n",
    "        for obs in self.gps_generator.obstacles:\n",
    "            dist = np.sqrt((self.position[0] - obs['position'][0])**2 + \n",
    "                         (self.position[1] - obs['position'][1])**2)\n",
    "            safe_dist = dist - obs['radius']\n",
    "            min_safe_distance = min(min_safe_distance, safe_dist)\n",
    "            \n",
    "            if dist < obs['radius']:\n",
    "                safety_penalty = -100\n",
    "            elif dist < obs['radius'] + 20:\n",
    "                safety_penalty = min(safety_penalty, -20 * (1 - (dist - obs['radius']) / 20))\n",
    "        \n",
    "        if min_safe_distance > 30:\n",
    "            safety_bonus = 1.0\n",
    "        else:\n",
    "            safety_bonus = 0\n",
    "        \n",
    "        battery_penalty = -10 if self.battery < 10 else 0\n",
    "        \n",
    "        time_penalty = -0.01\n",
    "        \n",
    "        return dist_reward + wp_reward + progress_reward + safety_penalty + battery_penalty + safety_bonus + time_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c59a91-f666-4c5d-b6f0-f0721a37bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:    \n",
    "    def __init__(self, state_dim: int, action_dim: int, lr: float = 1e-3):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.memory = ReplayBuffer(100000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.001\n",
    "        \n",
    "    def select_action(self, state, noise=0.1):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.actor(state).cpu().data.numpy()[0]\n",
    "        if noise > 0:\n",
    "            action += np.random.normal(0, noise, size=action.shape)\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        done = torch.FloatTensor(done).unsqueeze(1).to(device)\n",
    "        \n",
    "        next_action = self.actor_target(next_state)\n",
    "        target_q = self.critic_target(next_state, next_action)\n",
    "        target_q = reward + (1 - done) * self.gamma * target_q\n",
    "        current_q = self.critic(state, action)\n",
    "        \n",
    "        critic_loss = F.mse_loss(current_q, target_q.detach())\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self._soft_update(self.actor_target, self.actor)\n",
    "        self._soft_update(self.critic_target, self.critic)\n",
    "    \n",
    "    def _soft_update(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028515e8",
   "metadata": {},
   "source": [
    "Professor, for my HIL interpretation I let the model to manually leave the collision trajectory by checking proximity to collision targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616d2b4-f1d0-4c55-b202-ed69a5202d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanInLoopDDPG(DDPGAgent):    \n",
    "    def __init__(self, state_dim: int, action_dim: int, lr: float = 1e-3):\n",
    "        super().__init__(state_dim, action_dim, lr)\n",
    "        self.authority_net = AuthorityNetwork(state_dim).to(device)\n",
    "        self.authority_optimizer = optim.Adam(self.authority_net.parameters(), lr=lr/10)\n",
    "        self.human_simulator = self._create_human_simulator()\n",
    "        self.authority_memory = ReplayBuffer(10000)\n",
    "        \n",
    "    def _create_human_simulator(self):\n",
    "        def human_action(state, env_info):\n",
    "            danger = state[11]\n",
    "            min_obs_dist = state[11] * 50  \n",
    "            \n",
    "            if danger > 0.7 or (min_obs_dist < 30 and min_obs_dist > 0):\n",
    "                current_x, current_y = state[0] * 1000, state[1] * 1000\n",
    "                \n",
    "                best_action = None\n",
    "                best_safety = -float('inf')\n",
    "                \n",
    "                for angle in np.linspace(0, 2*np.pi, 8):\n",
    "                    test_dx = np.cos(angle) * 0.8\n",
    "                    test_dy = np.sin(angle) * 0.8\n",
    "                    \n",
    "                    new_x = current_x + test_dx * 10\n",
    "                    new_y = current_y + test_dy * 10\n",
    "                    \n",
    "                    safety_score = 0\n",
    "                    if hasattr(env_info, 'gps_generator'):\n",
    "                        for obs in env_info.gps_generator.obstacles:\n",
    "                            dist = np.sqrt((new_x - obs['position'][0])**2 + \n",
    "                                         (new_y - obs['position'][1])**2)\n",
    "                            safety_score += max(0, dist - obs['radius'])\n",
    "                    \n",
    "                    if safety_score > best_safety:\n",
    "                        best_safety = safety_score\n",
    "                        best_action = np.array([test_dx, test_dy, 0.1])\n",
    "                \n",
    "                return best_action if best_action is not None else np.array([0, 0, 0.2])\n",
    "            return None\n",
    "        \n",
    "        return human_action\n",
    "    \n",
    "    def select_action(self, state, env_info=None, noise=0.1):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        ai_action = self.actor(state_tensor).cpu().data.numpy()[0]\n",
    "        if noise > 0:\n",
    "            ai_action += np.random.normal(0, noise * 0.5, size=ai_action.shape)\n",
    "        ai_action = np.clip(ai_action, -1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            authority = self.authority_net(state_tensor).cpu().data.numpy()[0, 0]\n",
    "        \n",
    "        danger = state[11]\n",
    "        human_action = self.human_simulator(state, env_info)\n",
    "        \n",
    "        if human_action is not None and (danger > 0.6 or authority < 0.3):\n",
    "            final_action = human_action\n",
    "            control_mode = 'human'\n",
    "            actual_authority = 0.0\n",
    "        elif human_action is not None and danger > 0.3:\n",
    "            blend_factor = 1 - danger \n",
    "            final_action = blend_factor * ai_action + (1 - blend_factor) * human_action\n",
    "            control_mode = 'shared'\n",
    "            actual_authority = blend_factor\n",
    "        else:\n",
    "            final_action = ai_action\n",
    "            control_mode = 'ai'\n",
    "            actual_authority = 1.0\n",
    "        \n",
    "        return final_action, actual_authority, control_mode\n",
    "    \n",
    "    def update_authority(self, state, authority, reward, next_state, done, collision=False):\n",
    "        self.authority_memory.push(state, authority, reward, next_state, done)\n",
    "        \n",
    "        if len(self.authority_memory) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        batch_state, batch_authority, batch_reward, batch_next_state, batch_done = \\\n",
    "            self.authority_memory.sample(self.batch_size)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(batch_state).to(device)\n",
    "        authority_tensor = torch.FloatTensor(batch_authority).unsqueeze(1).to(device)\n",
    "        reward_tensor = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
    "        next_state_tensor = torch.FloatTensor(batch_next_state).to(device)\n",
    "        done_tensor = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
    "        \n",
    "        predicted_authority = self.authority_net(state_tensor)\n",
    "        \n",
    "        danger_current = state_tensor[:, 11].unsqueeze(1)\n",
    "        danger_next = next_state_tensor[:, 11].unsqueeze(1)\n",
    "        \n",
    "        target_authority = torch.sigmoid((0.5 - danger_next) * 10)\n",
    "        \n",
    "        reward_signal = torch.sigmoid(reward_tensor / 10)\n",
    "        target_authority = 0.7 * target_authority + 0.3 * reward_signal\n",
    "        \n",
    "        if collision:\n",
    "            target_authority = torch.zeros_like(target_authority)\n",
    "        \n",
    "        authority_loss = F.mse_loss(predicted_authority, target_authority.detach())\n",
    "        \n",
    "        self.authority_optimizer.zero_grad()\n",
    "        authority_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.authority_net.parameters(), 1.0)\n",
    "        self.authority_optimizer.step()\n",
    "\n",
    "    def pretrain_authority_network(self, env, episodes=20):\n",
    "        print(\"Pre-training authority network...\")\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                danger = state[11]\n",
    "                \n",
    "                if danger > 0.7:\n",
    "                    target_authority = 0.0 \n",
    "                elif danger > 0.4:\n",
    "                    target_authority = 0.3 \n",
    "                elif danger > 0.2:\n",
    "                    target_authority = 0.6\n",
    "                else:\n",
    "                    target_authority = 0.95 \n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                predicted = self.authority_net(state_tensor)\n",
    "                target = torch.FloatTensor([[target_authority]]).to(device)\n",
    "                \n",
    "                loss = F.mse_loss(predicted, target)\n",
    "                self.authority_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.authority_optimizer.step()\n",
    "                \n",
    "                action = np.random.uniform(-1, 1, size=3)\n",
    "                next_state, _, done, _ = env.step(action)\n",
    "                state = next_state\n",
    "        \n",
    "        print(f\"Pre-training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a7a63-8fe8-41b6-a123-0b5b6b84c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpg(env, agent, episodes=100):\n",
    "    rewards_history = []\n",
    "    success_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state, noise=0.1)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        success_history.append(info.get('mission_complete', False))\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            recent_success_rate = np.mean(success_history[-10:]) if len(success_history) >= 10 else np.mean(success_history)\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward:.2f}, \"\n",
    "                  f\"Waypoints: {info['waypoints_reached']}/{info['total_waypoints']}, \"\n",
    "                  f\"Success Rate: {recent_success_rate*100:.1f}%\")\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "def train_human_in_loop_ddpg(env, agent, episodes=100):\n",
    "    rewards_history = []\n",
    "    authority_history = []\n",
    "    control_mode_stats = {'ai': 0, 'shared': 0, 'human': 0}\n",
    "    collision_history = []\n",
    "    success_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_authority = []\n",
    "        episode_collisions = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, authority, control_mode = agent.select_action(state, env, noise=0.1)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            collision = False\n",
    "            for obs in env.gps_generator.obstacles:\n",
    "                dist = np.sqrt((env.position[0] - obs['position'][0])**2 + \n",
    "                             (env.position[1] - obs['position'][1])**2)\n",
    "                if dist < obs['radius']:\n",
    "                    collision = True\n",
    "                    episode_collisions += 1\n",
    "                    break\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            agent.update()\n",
    "            agent.update_authority(state, authority, reward, next_state, done, collision)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_authority.append(authority)\n",
    "            control_mode_stats[control_mode] += 1\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        authority_history.append(np.mean(episode_authority))\n",
    "        collision_history.append(episode_collisions)\n",
    "        success_history.append(info.get('mission_complete', False))\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            recent_success_rate = np.mean(success_history[-10:]) if len(success_history) >= 10 else np.mean(success_history)\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward:.2f}, \"\n",
    "                  f\"Avg Authority: {np.mean(episode_authority):.2f}, \"\n",
    "                  f\"Waypoints: {info['waypoints_reached']}/{info['total_waypoints']}, \"\n",
    "                  f\"Success Rate: {recent_success_rate*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nTotal collisions over {episodes} episodes: {sum(collision_history)}\")\n",
    "    print(f\"Final success rate: {np.mean(success_history)*100:.1f}%\")\n",
    "    \n",
    "    return rewards_history, authority_history, control_mode_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5dafe2-0b98-4f16-be16-6b36a44639f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_safety_metrics(env, agent, n_episodes=20):\n",
    "    metrics = {\n",
    "        'collisions': 0,\n",
    "        'near_misses': 0,\n",
    "        'avg_min_obstacle_dist': [],\n",
    "        'danger_time_ratio': [],\n",
    "        'mission_success_rate': 0,\n",
    "        'avg_battery_remaining': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_min_distances = []\n",
    "        episode_danger_time = 0\n",
    "        total_timesteps = 0\n",
    "        \n",
    "        while not done:\n",
    "            if isinstance(agent, HumanInLoopDDPG):\n",
    "                action, _, _ = agent.select_action(state, env, noise=0)\n",
    "            else:\n",
    "                action = agent.select_action(state, noise=0)\n",
    "            \n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            \n",
    "            min_dist = float('inf')\n",
    "            for obs in env.gps_generator.obstacles:\n",
    "                dist = np.sqrt((env.position[0] - obs['position'][0])**2 + \n",
    "                             (env.position[1] - obs['position'][1])**2)\n",
    "                actual_dist = dist - obs['radius']\n",
    "                min_dist = min(min_dist, actual_dist)\n",
    "                \n",
    "                if actual_dist < 0:\n",
    "                    metrics['collisions'] += 1\n",
    "                elif actual_dist < 20:\n",
    "                    metrics['near_misses'] += 1\n",
    "            \n",
    "            episode_min_distances.append(min_dist)\n",
    "            if state[11] > 0.5:  # Danger level\n",
    "                episode_danger_time += 1\n",
    "            total_timesteps += 1\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        metrics['avg_min_obstacle_dist'].append(np.mean(episode_min_distances))\n",
    "        metrics['danger_time_ratio'].append(episode_danger_time / total_timesteps)\n",
    "        metrics['avg_battery_remaining'].append(env.battery)\n",
    "        \n",
    "        if env.current_waypoint_idx >= len(env.gps_generator.waypoints):\n",
    "            metrics['mission_success_rate'] += 1\n",
    "    \n",
    "    metrics['mission_success_rate'] /= n_episodes\n",
    "    metrics['avg_min_obstacle_dist'] = np.mean(metrics['avg_min_obstacle_dist'])\n",
    "    metrics['danger_time_ratio'] = np.mean(metrics['danger_time_ratio'])\n",
    "    metrics['avg_battery_remaining'] = np.mean(metrics['avg_battery_remaining'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compare_models_performance(env, ddpg_agent, hil_agent):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Evaluating Model Safety and Performance...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    ddpg_metrics = evaluate_safety_metrics(env, ddpg_agent)\n",
    "    hil_metrics = evaluate_safety_metrics(env, hil_agent)\n",
    "    \n",
    "    print(\"\\nSafety Metrics Comparison:\")\n",
    "    print(f\"{'Metric':<30} {'Standard DDPG':<15} {'HIL-DDPG':<15} {'Improvement':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    metrics_to_compare = [\n",
    "        ('Collisions', 'collisions', 'lower'),\n",
    "        ('Near Misses', 'near_misses', 'lower'),\n",
    "        ('Avg Min Obstacle Dist (m)', 'avg_min_obstacle_dist', 'higher'),\n",
    "        ('Danger Time Ratio', 'danger_time_ratio', 'lower'),\n",
    "        ('Mission Success Rate', 'mission_success_rate', 'higher'),\n",
    "        ('Avg Battery Remaining (%)', 'avg_battery_remaining', 'higher')\n",
    "    ]\n",
    "    \n",
    "    for name, key, better in metrics_to_compare:\n",
    "        ddpg_val = ddpg_metrics[key]\n",
    "        hil_val = hil_metrics[key]\n",
    "        \n",
    "        if better == 'lower':\n",
    "            improvement = (ddpg_val - hil_val) / (ddpg_val + 1e-6) * 100\n",
    "        else:\n",
    "            improvement = (hil_val - ddpg_val) / (ddpg_val + 1e-6) * 100\n",
    "        \n",
    "        print(f\"{name:<30} {ddpg_val:<15.2f} {hil_val:<15.2f} {improvement:>14.1f}%\")\n",
    "    \n",
    "    return ddpg_metrics, hil_metrics\n",
    "\n",
    "def analyze_authority_patterns(env, hil_agent, n_episodes=10):\n",
    "    authority_data = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, authority, control_mode = hil_agent.select_action(state, env, noise=0)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            \n",
    "            authority_data.append({\n",
    "                'authority': authority,\n",
    "                'danger_level': state[11],\n",
    "                'obstacle_proximity': next_state[11] * 50,\n",
    "                'battery': state[10] * 100,\n",
    "                'distance_to_target': np.sqrt((state[3] - state[0])**2 + \n",
    "                                            (state[4] - state[1])**2) * 1000,\n",
    "                'altitude': state[2] * 100,\n",
    "                'wind_speed': state[9] * 20,\n",
    "                'control_mode': control_mode\n",
    "            })\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "    df = pd.DataFrame(authority_data)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    factors = ['danger_level', 'obstacle_proximity', 'battery', \n",
    "               'distance_to_target', 'altitude', 'wind_speed']\n",
    "    \n",
    "    for i, factor in enumerate(factors):\n",
    "        axes[i].scatter(df[factor], df['authority'], alpha=0.5)\n",
    "        axes[i].set_xlabel(factor.replace('_', ' ').title())\n",
    "        axes[i].set_ylabel('AI Authority')\n",
    "        axes[i].set_ylim(-0.1, 1.1)\n",
    "        \n",
    "        z = np.polyfit(df[factor], df['authority'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(df[factor].min(), df[factor].max(), 100)\n",
    "        axes[i].plot(x_line, p(x_line), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.suptitle('Authority Allocation Patterns Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAuthority Correlation Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    for factor in factors:\n",
    "        corr = df['authority'].corr(df[factor])\n",
    "        print(f\"{factor.replace('_', ' ').title():<25}: {corr:>6.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_research_plots(trajectory_data):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    sample_episode = trajectory_data[trajectory_data['episode'] == 0]\n",
    "    scatter = ax1.scatter(sample_episode['x'], sample_episode['y'], \n",
    "                         c=sample_episode['danger_level'], cmap='YlOrRd', \n",
    "                         s=20, alpha=0.7)\n",
    "    ax1.plot(sample_episode['x'], sample_episode['y'], 'b-', alpha=0.3)\n",
    "    ax1.set_xlabel('X Position (m)')\n",
    "    ax1.set_ylabel('Y Position (m)')\n",
    "    ax1.set_title('Sample Drone Trajectory with Danger Levels')\n",
    "    plt.colorbar(scatter, ax=ax1, label='Danger Level')\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(trajectory_data['danger_level'], bins=50, edgecolor='black')\n",
    "    ax2.set_xlabel('Danger Level')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Danger Levels in Training Data')\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    for ep in range(min(5, trajectory_data['episode'].max())):\n",
    "        ep_data = trajectory_data[trajectory_data['episode'] == ep]\n",
    "        ax3.plot(ep_data['timestep'], ep_data['battery'], \n",
    "                label=f'Episode {ep}', alpha=0.7)\n",
    "    ax3.set_xlabel('Timestep')\n",
    "    ax3.set_ylabel('Battery Level (%)')\n",
    "    ax3.set_title('Battery Consumption Patterns')\n",
    "    ax3.legend()\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.scatter(trajectory_data['min_obstacle_distance'], \n",
    "               trajectory_data['danger_level'], alpha=0.3, s=10)\n",
    "    ax4.set_xlabel('Minimum Obstacle Distance (m)')\n",
    "    ax4.set_ylabel('Danger Level')\n",
    "    ax4.set_title('Danger Level vs Obstacle Proximity')\n",
    "    ax4.set_xlim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('research_analysis_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def perform_statistical_analysis(ddpg_rewards, hil_rewards):\n",
    "    from scipy import stats\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Statistical Analysis for Research Paper\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(f\"{'Metric':<20} {'DDPG':<20} {'HIL-DDPG':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    metrics = [\n",
    "        ('Mean Reward', np.mean, '.2f'),\n",
    "        ('Std Deviation', np.std, '.2f'),\n",
    "        ('Max Reward', np.max, '.2f'),\n",
    "        ('Min Reward', np.min, '.2f'),\n",
    "        ('Median Reward', np.median, '.2f')\n",
    "    ]\n",
    "    \n",
    "    for name, func, fmt in metrics:\n",
    "        ddpg_val = func(ddpg_rewards[-20:])  # Last 20 episodes\n",
    "        hil_val = func(hil_rewards[-20:])\n",
    "        print(f\"{name:<20} {ddpg_val:<20{fmt}} {hil_val:<20{fmt}}\")\n",
    "    \n",
    "    print(\"\\n\\nStatistical Tests:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(ddpg_rewards[-20:], hil_rewards[-20:])\n",
    "    print(f\"Independent t-test: t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "    \n",
    "    u_stat, u_p_value = stats.mannwhitneyu(ddpg_rewards[-20:], hil_rewards[-20:])\n",
    "    print(f\"Mann-Whitney U test: U={u_stat:.3f}, p={u_p_value:.4f}\")\n",
    "    \n",
    "    pooled_std = np.sqrt((np.std(ddpg_rewards[-20:])**2 + np.std(hil_rewards[-20:])**2) / 2)\n",
    "    cohens_d = (np.mean(hil_rewards[-20:]) - np.mean(ddpg_rewards[-20:])) / pooled_std\n",
    "    print(f\"Cohen's d effect size: {cohens_d:.3f}\")\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"✓ Statistically significant difference between DDPG and HIL-DDPG (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"✗ No statistically significant difference found (p ≥ 0.05)\")\n",
    "    \n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    \n",
    "    print(f\"✓ Effect size is {effect} (|d| = {abs(cohens_d):.3f})\")\n",
    "\n",
    "def generate_latex_tables(ddpg_metrics, hil_metrics):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LaTeX Tables for Paper\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\n% Performance Comparison Table\")\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\caption{Safety and Performance Metrics Comparison}\")\n",
    "    print(\"\\\\label{tab:performance}\")\n",
    "    print(\"\\\\begin{tabular}{lrrr}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"Metric & Standard DDPG & HIL-DDPG & Improvement (\\\\%) \\\\\\\\\")\n",
    "    print(\"\\\\hline\")\n",
    "    \n",
    "    metrics = [\n",
    "        ('Collisions', 'collisions', 'lower'),\n",
    "        ('Near Misses', 'near_misses', 'lower'),\n",
    "        ('Avg. Min. Obstacle Dist. (m)', 'avg_min_obstacle_dist', 'higher'),\n",
    "        ('Mission Success Rate', 'mission_success_rate', 'higher'),\n",
    "    ]\n",
    "    \n",
    "    for name, key, better in metrics:\n",
    "        ddpg_val = ddpg_metrics[key]\n",
    "        hil_val = hil_metrics[key]\n",
    "        \n",
    "        if better == 'lower':\n",
    "            improvement = (ddpg_val - hil_val) / (ddpg_val + 1e-6) * 100\n",
    "        else:\n",
    "            improvement = (hil_val - ddpg_val) / (ddpg_val + 1e-6) * 100\n",
    "        \n",
    "        print(f\"{name} & {ddpg_val:.2f} & {hil_val:.2f} & {improvement:.1f} \\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee42d5d-4578-4f8e-a1ba-0d28e44723ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Drone Navigation Environment...\")\n",
    "    \n",
    "gps_gen = DroneGPSDataGenerator(area_size=(1000, 1000), \n",
    "                               n_obstacles=5, n_waypoints=3)\n",
    "\n",
    "print(\"\\nGenerating mock GPS trajectory data...\")\n",
    "trajectory_data = gps_gen.generate_trajectory_data(n_episodes=50)\n",
    "trajectory_data.to_csv('drone_gps_data.csv', index=False)\n",
    "print(f\"Generated {len(trajectory_data)} GPS data points\")\n",
    "print(f\"Data shape: {trajectory_data.shape}\")\n",
    "print(f\"Columns: {list(trajectory_data.columns)}\")\n",
    "\n",
    "env = DroneNavigationEnv(gps_gen)\n",
    "\n",
    "print(\"\\nVisualizing environment...\")\n",
    "visualize_environment(env)\n",
    "\n",
    "state_dim = env.state_dim\n",
    "action_dim = env.action_dim\n",
    "\n",
    "print(f\"\\nState dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Standard DDPG Agent...\")\n",
    "print(\"=\"*50)\n",
    "ddpg_agent = DDPGAgent(state_dim, action_dim)\n",
    "ddpg_rewards = train_ddpg(env, ddpg_agent, episodes=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Human-in-the-Loop DDPG Agent...\")\n",
    "print(\"=\"*50)\n",
    "hil_agent = HumanInLoopDDPG(state_dim, action_dim)\n",
    "\n",
    "hil_agent.pretrain_authority_network(env, episodes=20)\n",
    "\n",
    "hil_rewards, hil_authority, control_stats = train_human_in_loop_ddpg(\n",
    "    env, hil_agent, episodes=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete! Final Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Standard DDPG - Final avg reward: {np.mean(ddpg_rewards[-10:]):.2f}\")\n",
    "print(f\"HIL-DDPG - Final avg reward: {np.mean(hil_rewards[-10:]):.2f}\")\n",
    "print(f\"HIL-DDPG - Final avg authority: {np.mean(hil_authority[-10:]):.2f}\")\n",
    "print(f\"Control mode distribution: {control_stats}\")\n",
    "\n",
    "print(\"\\nPlotting training results...\")\n",
    "plot_training_results(ddpg_rewards, hil_rewards, hil_authority)\n",
    "\n",
    "print(\"\\nSaving trained models...\")\n",
    "torch.save({\n",
    "    'actor_state_dict': ddpg_agent.actor.state_dict(),\n",
    "    'critic_state_dict': ddpg_agent.critic.state_dict(),\n",
    "}, 'ddpg_model.pth')\n",
    "\n",
    "torch.save({\n",
    "    'actor_state_dict': hil_agent.actor.state_dict(),\n",
    "    'critic_state_dict': hil_agent.critic.state_dict(),\n",
    "    'authority_state_dict': hil_agent.authority_net.state_dict(),\n",
    "}, 'hil_ddpg_model.pth')\n",
    "\n",
    "print(\"\\nModels saved successfully!\")\n",
    "print(\"\\nExperiment complete. You can now analyze the results for your paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38372f5-9d13-4882-adef-60249c8ee1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nPerforming detailed evaluation for research paper...\")\n",
    "    \n",
    "# Compare models\n",
    "ddpg_metrics, hil_metrics = compare_models_performance(env, ddpg_agent, hil_agent)\n",
    "\n",
    "# Analyze authority patterns\n",
    "print(\"\\n\\nAnalyzing Authority Allocation Patterns...\")\n",
    "authority_df = analyze_authority_patterns(env, hil_agent)\n",
    "\n",
    "# Statistical analysis\n",
    "perform_statistical_analysis(ddpg_rewards, hil_rewards)\n",
    "\n",
    "# Generate research plots\n",
    "print(\"\\n\\nGenerating research quality plots...\")\n",
    "generate_research_plots(trajectory_data)\n",
    "\n",
    "# Generate LaTeX tables\n",
    "generate_latex_tables(ddpg_metrics, hil_metrics)\n",
    "\n",
    "print(\"\\n\\nAll analyses complete! Ready for paper submission.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
